{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign the feature names within ad.names to the columns of ad.data, and create the feature matrix and target vector while doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quang\\AppData\\Local\\Temp\\ipykernel_21244\\2390612365.py:5: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('../datasets/ads/ad.data')\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd     \n",
    "\n",
    "# Import the dataset.\n",
    "dataset = pd.read_csv('../datasets/ads/ad.data')\n",
    "\n",
    "# Assign the feature names to the dataset using the .names file.\n",
    "with open('../datasets/ads/adNames.txt', 'r') as file:\n",
    "    feature_names = [line.split(':')[0] for line in file.readlines()]\n",
    "    \n",
    "# Create the feature matrix by dropping the last column (which is the target\n",
    "# variable column), and assign the names to the feature matrix.\n",
    "feature_matrix = dataset.drop(['ad.'], axis=1)\n",
    "feature_matrix.columns = feature_names\n",
    "\n",
    "# Create the target vector by taking the last entry of the dataset and encode\n",
    "# the target vector.\n",
    "target_vector = dataset['ad.'].map({'ad.': 1, 'nonad.': 0})\n",
    "\n",
    "# Create the named dataset; the dataset with named features and including the \n",
    "# target_vector. exp_df is meant only to use to export the data; no other\n",
    "# operations should be done on this dataframe.\n",
    "exp_df = feature_matrix\n",
    "exp_df['ads'] = target_vector\n",
    "\n",
    "# Export the full_df\n",
    "# exp_df.to_csv('full_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean the data; note that missing values are listed as \"?\" because whoever put together this dataset is a moron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Find missing values\n",
    "################################################################################\n",
    "\n",
    "# Locate all non-numeric values and remove them.\n",
    "numeric_df = exp_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Plot a histogram of all the continuous variables.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Height uncleaned histogram\n",
    "# plt.figure(1, figsize=(10,15))\n",
    "# plt.subplot(3, 1, 1)\n",
    "# plt.hist(numeric_df['height'])\n",
    "# plt.xlabel('Height, in pixels')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.title('Histogram of Height')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Width uncleaned histogram\n",
    "# plt.subplot(3, 1, 2)\n",
    "# plt.hist(numeric_df['width'])\n",
    "# plt.xlabel('Width, in pixels')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.title('Histogram of Width')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Aratio uncleaned histogram\n",
    "# plt.subplot(3, 1, 3)\n",
    "# plt.hist(numeric_df['aratio'])\n",
    "# plt.xlabel('Aratio')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.title('Histogram of Aratio')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate all columns with missing values.\n",
    "missings = numeric_df.loc[:, numeric_df.isnull().any()]\n",
    "\n",
    "################################################################################\n",
    "# Use KNN imputation to fill in the missing values.\n",
    "################################################################################\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "# TODO\n",
    "# Test different scalers\n",
    "# Use RobustScaler to avoid pruning outliers, since we can't easily determine \n",
    "# if they're actually outliers.\n",
    "scaler = RobustScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(numeric_df), columns=numeric_df.columns)\n",
    "\n",
    "# TODO\n",
    "# Test different n_neighbors; sqrt(n) works relatively well.\n",
    "# Use some percent of the data as the number of neighbors for the imputer.\n",
    "knn_imputer = KNNImputer(copy=False, n_neighbors = round(np.sqrt(numeric_df.shape[0])))\n",
    "\n",
    "# Scale and impute the dataset.\n",
    "scaled_and_imputed_nd = knn_imputer.fit_transform(data_scaled)\n",
    "\n",
    "# Unscale the dataset\n",
    "unscaled_and_imputed_nd = scaler.inverse_transform(scaled_and_imputed_nd)\n",
    "unscaled_and_imputed_df = pd.DataFrame(unscaled_and_imputed_nd)\n",
    "\n",
    "# Post process the \"aratio\" feature by rounding all values >= 0.5, and flooring\n",
    "# the rest.\n",
    "unscaled_and_imputed_df[3] = unscaled_and_imputed_df[3].apply(round)\n",
    "\n",
    "# Append ads to feature names\n",
    "feature_names.append('ad.')\n",
    "\n",
    "# Apply the feature names to the dataframe, again.\n",
    "unscaled_and_imputed_df.columns = feature_names\n",
    "\n",
    "# # Save the above under clean_df\n",
    "clean_df = unscaled_and_imputed_df\n",
    "# clean_df.to_csv('clean_df.csv')\n",
    "\n",
    "# # Plot the histograms again, this time of the cleaned values\n",
    "\n",
    "# # Height uncleaned histogram\n",
    "# plt.figure(2, figsize=(10,15))\n",
    "# plt.subplot(3, 1, 1)\n",
    "# plt.hist(clean_df['height'])\n",
    "# plt.xlabel('Height, in pixels')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.title('Histogram of Height')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Width uncleaned histogram\n",
    "# plt.subplot(3, 1, 2)\n",
    "# plt.hist(clean_df['width'])\n",
    "# plt.xlabel('Width, in pixels')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.title('Histogram of Width')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Aratio uncleaned histogram\n",
    "# plt.subplot(3, 1, 3)\n",
    "# plt.hist(clean_df['aratio'])\n",
    "# plt.xlabel('Aratio')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.title('Histogram of Aratio')\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conduct EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "#STEP 1: intial understanding of the clean dataset\n",
    "\n",
    "#target variable- image that is going to be classfied as ad or non-ad\n",
    "#this shows the class distribution of the target variable\n",
    "# lean_df['ad.'].value_counts().plot(kind='bar')\n",
    "# plt.xlabel('Ad.')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('The distribution of the target variable')\n",
    "#there is a imbalance between ad/ non-ad need to find a way to decrease class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Data quality assessment \n",
    "#-skipping for now since you did most of the cleaning already but we can come back to this step later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP 3: find features that have strong correlation with target variable\n",
    "# from scipy.stats import pointbiserialr\n",
    "\n",
    "# # Calculate the point-biserial correlation coefficient\n",
    "# corr1, p_value1 = pointbiserialr(clean_df['height'], clean_df['ad.'])\n",
    "# corr2, p_value2 = pointbiserialr(clean_df['width'], clean_df['ad.'])\n",
    "# corr3, p_value3 = pointbiserialr(clean_df['aratio'], clean_df['ad.'])\n",
    "\n",
    "\n",
    "# print(\"Point-biserial correlation coefficient of height:\", corr1)\n",
    "# print(\"p-value:\", p_value1)\n",
    "# print(\"Point-biserial correlation coefficient of width:\", corr2)\n",
    "# print(\"p-value:\", p_value2)\n",
    "# print(\"Point-biserial correlation coefficient of aratio:\", corr3)\n",
    "# print(\"p-value:\", p_value3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ancurl*com: Chi-squared = 1085.6134\n",
      "url*ads: Chi-squared = 886.2597\n",
      "ancurl*click: Chi-squared = 846.8578\n",
      "alt*click: Chi-squared = 646.6715\n",
      "ancurl*redirect: Chi-squared = 633.3237\n",
      "ancurl*ng: Chi-squared = 618.3402\n",
      "alt*click+here: Chi-squared = 615.3102\n",
      "alt*here: Chi-squared = 593.8869\n",
      "ancurl*adid: Chi-squared = 533.5903\n",
      "ancurl*type: Chi-squared = 533.5903\n",
      "ancurl*event: Chi-squared = 475.3352\n",
      "ancurl*ng+type: Chi-squared = 468.8835\n",
      "ancurl*runid: Chi-squared = 468.8835\n",
      "ancurl*event+ng: Chi-squared = 468.8835\n",
      "ancurl*type+click: Chi-squared = 468.8835\n",
      "ancurl*profileid: Chi-squared = 468.8835\n",
      "ancurl*2f: Chi-squared = 462.4361\n",
      "ancurl*redirect+http: Chi-squared = 455.9930\n",
      "ancurl*http+2f: Chi-squared = 455.9930\n",
      "ancurl*http+www: Chi-squared = 444.4478\n",
      "alt*for: Chi-squared = 364.4912\n",
      "ancurl*2f+2fwww: Chi-squared = 308.9901\n",
      "ancurl*2fwww: Chi-squared = 308.9901\n",
      "alt*here+for: Chi-squared = 294.6246\n",
      "ancurl*familyid: Chi-squared = 292.7183\n",
      "ancurl*groupid: Chi-squared = 292.7183\n",
      "ancurl*tagvalues: Chi-squared = 292.7183\n",
      "url*ad: Chi-squared = 270.1179\n",
      "ancurl*click+profileid: Chi-squared = 264.7194\n",
      "ancurl*ad: Chi-squared = 256.9030\n",
      "ancurl*cid: Chi-squared = 248.4464\n",
      "ancurl*adclick: Chi-squared = 235.8330\n",
      "ancurl*bin: Chi-squared = 233.8696\n",
      "url*ng: Chi-squared = 216.9425\n",
      "ancurl*groupid+1: Chi-squared = 210.6536\n",
      "ancurl*1+familyid: Chi-squared = 210.6536\n",
      "origurl*bin: Chi-squared = 209.2124\n",
      "ancurl*site: Chi-squared = 200.7714\n",
      "ancurl*click+runid: Chi-squared = 191.8108\n",
      "url*ads+media: Chi-squared = 179.2689\n",
      "caption*click: Chi-squared = 176.5648\n",
      "ancurl*cat: Chi-squared = 176.1855\n",
      "url*image+ng: Chi-squared = 173.0040\n",
      "url*advertising: Chi-squared = 173.0040\n",
      "ancurl*clickthru: Chi-squared = 166.7431\n",
      "ancurl*image: Chi-squared = 165.3821\n",
      "caption*here: Chi-squared = 164.4945\n",
      "url*media+images: Chi-squared = 160.4863\n",
      "ancurl*clickid: Chi-squared = 160.4863\n",
      "ancurl*edition: Chi-squared = 160.4863\n"
     ]
    }
   ],
   "source": [
    "#Using Chi-squared test to assess asociations between binary features and target feature\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_results = {}\n",
    "# Exclude the first four columns and the target column\n",
    "for col in clean_df.columns[4:]:  # Start from the 5th column\n",
    "    if col != 'ad.':  # Exclude the target variable itself\n",
    "        contingency_table = pd.crosstab(clean_df[col], clean_df['ad.'])\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "        chi2_results[col] = chi2  # Store the Chi-squared statistic\n",
    "\n",
    "# Sort features by Chi-squared statistic\n",
    "sorted_chi2_results = sorted(chi2_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top n features with the strongest correlation\n",
    "top_feats = sorted_chi2_results[:50]  # Get the top n features\n",
    "for feature, chi2_value in top_feats:\n",
    "    print(f\"{feature}: Chi-squared = {chi2_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract feature names and create a DataFrame\n",
    "# top_features = [feature for feature, _ in top_30_chi2_results]\n",
    "# top_features.append('ad.')  # Include target variable for correlation\n",
    "\n",
    "# # Subset the original DataFrame\n",
    "# subset_df = clean_df[top_features]\n",
    "\n",
    "# # Calculate correlation matrix for the subset\n",
    "# correlation_matrix = subset_df.corr()\n",
    "\n",
    "# # Create heatmap to show correlation between top features and target (useful for PCA later)\n",
    "# plt.figure(figsize=(36, 30))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "# plt.title('Heatmap of Top 30 Features Correlating with Target Variable')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP 4: Visualizing numerical and categorical data to find outliers\n",
    "\n",
    "# #numerical data visualization\n",
    "# # Select the first three numerical columns\n",
    "# numerical_columns = clean_df.columns[:3]\n",
    "\n",
    "# # Generate box plots for the first three numerical columns \n",
    "# plt.figure(figsize=(15, 5))  # Adjust figure size as necessary\n",
    "# for i, col in enumerate(numerical_columns, 1):\n",
    "#     plt.subplot(1, 3, i)  # Arrange plots in a single row\n",
    "#     sns.boxplot(data=clean_df, y=col)\n",
    "#     plt.title(f'Box Plot of {col}')\n",
    "#     plt.ylabel(col)\n",
    "\n",
    "# plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate summary statistics for the first three numerical columns\n",
    "# summary_statistics = clean_df[numerical_columns].describe()\n",
    "# print(summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot frequency distribution for each feature\n",
    "# plt.figure(figsize=(25, 30))  # Adjust figure size as necessary\n",
    "# for i, feature in enumerate(top_features, 1):\n",
    "#     plt.subplot(10, 6, i)  # Arrange plots in a 6x5 grid\n",
    "#     ax = sns.histplot(subset_df[feature], discrete=True) # Use histplot for continuous features\n",
    "    \n",
    "#     # Add labels on top of the bars\n",
    "#     for container in ax.containers:\n",
    "#         ax.bar_label(container, label_type=\"edge\")  # Set label position to 'edge' or 'center'\n",
    "        \n",
    "#     plt.title(feature)\n",
    "#     plt.xlabel('Value')\n",
    "#     plt.ylabel('Frequency')\n",
    "    \n",
    "\n",
    "# plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "# plt.show()\n",
    "\n",
    "#results show that there is a class imbalance with most categorical classes being 0 and less 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the top n chi-squared features, generate 4 svm models with different kernels to test the performance of each kernel, then, expand upon this; ensemble SVM? Bagging? We'll see.\n",
    "#### How important are chi squared and correlation to model performance? We'll see.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Shuffle the dataframe. \n",
    "shuffled_df = clean_df.sample(frac=1, random_state=69420).reset_index(drop=True)\n",
    "\n",
    "# Create the target vector, y, and feature matrix X.\n",
    "y = shuffled_df['ad.']\n",
    "X = shuffled_df.drop('ad.', axis=1)\n",
    "\n",
    "# Train on 85% of data, test on 10%, val on 5%.\n",
    "# Step 1: Split 85% for training and 15% for test+validation\n",
    "X_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.15, random_state=69420)\n",
    "\n",
    "# Step 2: Split temp_df into 10% test and 5% validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=1/3, random_state=69420)\n",
    "\n",
    "# Check the sizes to confirm\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Testing set: {len(X_test)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
